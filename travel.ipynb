{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "pd.set_option(\"display.max_rows\", 101)\n",
    "pd.set_option(\"display.max_columns\", 101)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"train/train_data.csv\")\n",
    "label=pd.read_csv(\"train/train_label.csv\")\n",
    "test=pd.read_csv(\"test/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#入力がずれているので調整\n",
    "train=train.iloc[1:,:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feature(sentence, tokenizer):\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    bases = [str(t.base_form) for t in tokens]\n",
    "    return bases\n",
    "\n",
    "def chara_feature(sentence, tokenizer):\n",
    "    bases = [i for i in sentence]\n",
    "    return bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from janome.tokenizer import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "feature_func = lambda x: word_feature(x, tokenizer)\n",
    "texts = pd.concat([train['text'],test[\"text\"]],axis=0)\n",
    "#word_counter = CountVectorizer(min_df=3, tokenizer=feature_func,max_features=100)\n",
    "#word_counter =TfidfVectorizer(max_features=25000, ngram_range=1, tokenizer=feature_func)\n",
    "tfidf = TfidfVectorizer(tokenizer=feature_func,max_features=500)\n",
    "tmp = tfidf.fit_transform(texts)\n",
    "\n",
    "feature_func = lambda x: chara_feature(x, tokenizer)\n",
    "tfidf = TfidfVectorizer(tokenizer=feature_func,max_features=500)\n",
    "tmp2 = tfidf.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_co=tmp[:train.shape[0]].todense()\n",
    "test_co=tmp[train.shape[0]:].todense()\n",
    "train_co2=tmp2[:train.shape[0]].todense()\n",
    "test_co2=tmp2[train.shape[0]:].todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "tmp=pd.concat([train,test],axis=0)\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "le.fit(tmp[\"place\"].unique())\n",
    "place2=le.transform(tmp[\"place\"]) \n",
    "\n",
    "le.fit(tmp[\"open_type\"].fillna(\"nan\").unique())\n",
    "open_type2=le.transform(tmp[\"open_type\"].fillna(\"nan\")) \n",
    "\n",
    "le.fit(tmp[\"open_hours_st\"].fillna(\"nan\").unique())\n",
    "open_hours_st2=le.transform(tmp[\"open_hours_st\"].fillna(\"nan\"))\n",
    "\n",
    "le.fit(tmp[\"open_days_ed\"].fillna(\"nan\").unique())\n",
    "open_days_ed2=le.transform(tmp[\"open_days_ed\"].fillna(\"nan\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"placenum\"]=place2[:train.shape[0]]\n",
    "test[\"placenum\"]=place2[train.shape[0]:]\n",
    "train[\"open_type2\"]=open_type2[:train.shape[0]]\n",
    "test[\"open_type2\"]=open_type2[train.shape[0]:]\n",
    "train[\"open_hours_st2\"]=open_hours_st2[:train.shape[0]]\n",
    "test[\"open_hours_st2\"]=open_hours_st2[train.shape[0]:]\n",
    "train[\"open_days_ed2\"]=open_days_ed2[:train.shape[0]]\n",
    "test[\"open_days_ed2\"]=open_days_ed2[train.shape[0]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp_train=pd.concat([train[[\"placenum\",\"open_allday\",'open_type2',\n",
    "#       'open_hours_st2', 'open_days_ed2']],pd.DataFrame(train_co),pd.DataFrame(train_co2)],axis=1)\n",
    "#mlp_test=pd.concat([test[[\"placenum\",\"open_allday\",'open_type2',\n",
    "#       'open_hours_st2', 'open_days_ed2']],pd.DataFrame(test_co),pd.DataFrame(test_co2)],axis=1)\n",
    "mlp_train=pd.concat([pd.DataFrame(train_co),pd.DataFrame(train_co2)],axis=1)\n",
    "mlp_test=pd.concat([pd.DataFrame(test_co),pd.DataFrame(test_co2)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lavel変換\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(label[\"Sports_Resort\"].unique())\n",
    "label2=le.transform(label[\"Sports_Resort\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmizuta/.pyenv/versions/anaconda3-5.2.0/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, GlobalAveragePooling2D,Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "#use mlp for staking\n",
    "def build_mlp_model(input_dim):\n",
    "    model_input = Input(shape=(input_dim,))\n",
    "    #model_input = Input(shape=(input_dim,), sparse=True)\n",
    "    x = Dense(192, activation='relu')(model_input)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(8)(x)\n",
    "    output=Activation('softmax')(x)\n",
    "    model = keras.Model(model_input, output)\n",
    "    #model.compile(\n",
    "    #    loss='mean_squared_error',\n",
    "    #    optimizer=keras.optimizers.Adam(lr=3e-3)\n",
    "    #)\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=build_mlp_model(mlp_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "Y = np_utils.to_categorical(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1005)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 192)               193152    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                12352     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 210,184\n",
      "Trainable params: 210,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3573 samples, validate on 894 samples\n",
      "Epoch 1/10\n",
      "3573/3573 [==============================] - 1s 182us/step - loss: 1.5669 - acc: 0.4344 - val_loss: 1.2016 - val_acc: 0.5347\n",
      "Epoch 2/10\n",
      "3573/3573 [==============================] - 1s 191us/step - loss: 0.8594 - acc: 0.6991 - val_loss: 0.6821 - val_acc: 0.7919\n",
      "Epoch 3/10\n",
      "3573/3573 [==============================] - 1s 211us/step - loss: 0.5782 - acc: 0.7996 - val_loss: 0.5565 - val_acc: 0.8076\n",
      "Epoch 4/10\n",
      "3573/3573 [==============================] - 0s 133us/step - loss: 0.4541 - acc: 0.8463 - val_loss: 0.5069 - val_acc: 0.8322\n",
      "Epoch 5/10\n",
      "3573/3573 [==============================] - 0s 131us/step - loss: 0.3976 - acc: 0.8631 - val_loss: 0.4917 - val_acc: 0.8333\n",
      "Epoch 6/10\n",
      "3573/3573 [==============================] - 0s 134us/step - loss: 0.3240 - acc: 0.8892 - val_loss: 0.4974 - val_acc: 0.8412\n",
      "Epoch 7/10\n",
      "3573/3573 [==============================] - 1s 141us/step - loss: 0.2799 - acc: 0.9004 - val_loss: 0.5306 - val_acc: 0.8311\n",
      "Epoch 8/10\n",
      "3573/3573 [==============================] - 0s 140us/step - loss: 0.2418 - acc: 0.9205 - val_loss: 0.5126 - val_acc: 0.8434\n",
      "Epoch 9/10\n",
      "3573/3573 [==============================] - 0s 130us/step - loss: 0.2254 - acc: 0.9244 - val_loss: 0.6326 - val_acc: 0.7897\n",
      "Epoch 10/10\n",
      "3573/3573 [==============================] - 0s 131us/step - loss: 0.1861 - acc: 0.9379 - val_loss: 0.5634 - val_acc: 0.8378\n",
      "Train on 3573 samples, validate on 894 samples\n",
      "Epoch 1/10\n",
      "3573/3573 [==============================] - 0s 135us/step - loss: 1.6727 - acc: 0.3893 - val_loss: 1.1764 - val_acc: 0.5436\n",
      "Epoch 2/10\n",
      "3573/3573 [==============================] - 0s 132us/step - loss: 0.9542 - acc: 0.6667 - val_loss: 0.7390 - val_acc: 0.7315\n",
      "Epoch 3/10\n",
      "3573/3573 [==============================] - 0s 133us/step - loss: 0.6347 - acc: 0.7862 - val_loss: 0.6439 - val_acc: 0.7651\n",
      "Epoch 4/10\n",
      "3573/3573 [==============================] - 0s 130us/step - loss: 0.5162 - acc: 0.8282 - val_loss: 0.5725 - val_acc: 0.7931\n",
      "Epoch 5/10\n",
      "3573/3573 [==============================] - 0s 130us/step - loss: 0.4034 - acc: 0.8657 - val_loss: 0.5463 - val_acc: 0.8076\n",
      "Epoch 6/10\n",
      "3573/3573 [==============================] - 0s 131us/step - loss: 0.3546 - acc: 0.8808 - val_loss: 0.5096 - val_acc: 0.8367\n",
      "Epoch 7/10\n",
      "3573/3573 [==============================] - 0s 133us/step - loss: 0.3080 - acc: 0.8936 - val_loss: 0.5009 - val_acc: 0.8378\n",
      "Epoch 8/10\n",
      "3573/3573 [==============================] - 0s 130us/step - loss: 0.2657 - acc: 0.9135 - val_loss: 0.5485 - val_acc: 0.8210\n",
      "Epoch 9/10\n",
      "3573/3573 [==============================] - 0s 132us/step - loss: 0.2408 - acc: 0.9194 - val_loss: 0.5641 - val_acc: 0.8233\n",
      "Epoch 10/10\n",
      "3573/3573 [==============================] - 0s 128us/step - loss: 0.2059 - acc: 0.9342 - val_loss: 0.5790 - val_acc: 0.8311\n",
      "Train on 3574 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      "3574/3574 [==============================] - 0s 130us/step - loss: 1.6553 - acc: 0.4309 - val_loss: 1.1569 - val_acc: 0.6260\n",
      "Epoch 2/10\n",
      "3574/3574 [==============================] - 0s 131us/step - loss: 0.8470 - acc: 0.7166 - val_loss: 0.6818 - val_acc: 0.7716\n",
      "Epoch 3/10\n",
      "3574/3574 [==============================] - 0s 132us/step - loss: 0.5370 - acc: 0.8240 - val_loss: 0.5684 - val_acc: 0.8063\n",
      "Epoch 4/10\n",
      "3574/3574 [==============================] - 0s 130us/step - loss: 0.4442 - acc: 0.8456 - val_loss: 0.5451 - val_acc: 0.8119\n",
      "Epoch 5/10\n",
      "3574/3574 [==============================] - 0s 131us/step - loss: 0.3617 - acc: 0.8788 - val_loss: 0.5398 - val_acc: 0.8119\n",
      "Epoch 6/10\n",
      "3574/3574 [==============================] - 0s 130us/step - loss: 0.2959 - acc: 0.9068 - val_loss: 0.5464 - val_acc: 0.8063\n",
      "Epoch 7/10\n",
      "3574/3574 [==============================] - 0s 132us/step - loss: 0.2670 - acc: 0.9116 - val_loss: 0.5079 - val_acc: 0.8231\n",
      "Epoch 8/10\n",
      "3574/3574 [==============================] - 0s 136us/step - loss: 0.2430 - acc: 0.9194 - val_loss: 0.5454 - val_acc: 0.8085\n",
      "Epoch 9/10\n",
      "3574/3574 [==============================] - 0s 131us/step - loss: 0.1837 - acc: 0.9438 - val_loss: 0.5390 - val_acc: 0.8287\n",
      "Epoch 10/10\n",
      "3574/3574 [==============================] - 0s 131us/step - loss: 0.1575 - acc: 0.9527 - val_loss: 0.6058 - val_acc: 0.8074\n",
      "Train on 3574 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      "3574/3574 [==============================] - 0s 130us/step - loss: 1.6168 - acc: 0.4194 - val_loss: 1.1771 - val_acc: 0.5588\n",
      "Epoch 2/10\n",
      "3574/3574 [==============================] - 0s 130us/step - loss: 0.8597 - acc: 0.7112 - val_loss: 0.6908 - val_acc: 0.7794\n",
      "Epoch 3/10\n",
      "3574/3574 [==============================] - 0s 132us/step - loss: 0.5508 - acc: 0.8092 - val_loss: 0.6554 - val_acc: 0.7816\n",
      "Epoch 4/10\n",
      "3574/3574 [==============================] - 0s 134us/step - loss: 0.4383 - acc: 0.8495 - val_loss: 0.6287 - val_acc: 0.8029\n",
      "Epoch 5/10\n",
      "3574/3574 [==============================] - 0s 129us/step - loss: 0.3633 - acc: 0.8788 - val_loss: 0.6689 - val_acc: 0.7626\n",
      "Epoch 6/10\n",
      "3574/3574 [==============================] - 0s 129us/step - loss: 0.3192 - acc: 0.8912 - val_loss: 0.5695 - val_acc: 0.8287\n",
      "Epoch 7/10\n",
      "3574/3574 [==============================] - 0s 129us/step - loss: 0.2525 - acc: 0.9172 - val_loss: 0.6404 - val_acc: 0.7906\n",
      "Epoch 8/10\n",
      "3574/3574 [==============================] - 0s 130us/step - loss: 0.2425 - acc: 0.9147 - val_loss: 0.7123 - val_acc: 0.8085\n",
      "Epoch 9/10\n",
      "3574/3574 [==============================] - 0s 132us/step - loss: 0.2198 - acc: 0.9287 - val_loss: 0.7794 - val_acc: 0.7716\n",
      "Epoch 10/10\n",
      "3574/3574 [==============================] - 0s 137us/step - loss: 0.1839 - acc: 0.9398 - val_loss: 0.6930 - val_acc: 0.8063\n",
      "Train on 3574 samples, validate on 893 samples\n",
      "Epoch 1/10\n",
      "3574/3574 [==============================] - 0s 139us/step - loss: 1.5972 - acc: 0.4354 - val_loss: 1.0207 - val_acc: 0.6473\n",
      "Epoch 2/10\n",
      "3574/3574 [==============================] - 0s 134us/step - loss: 0.8533 - acc: 0.6908 - val_loss: 0.8968 - val_acc: 0.6629\n",
      "Epoch 3/10\n",
      "3574/3574 [==============================] - 0s 131us/step - loss: 0.6310 - acc: 0.7801 - val_loss: 0.7309 - val_acc: 0.7402\n",
      "Epoch 4/10\n",
      "3574/3574 [==============================] - 0s 134us/step - loss: 0.4999 - acc: 0.8285 - val_loss: 0.6025 - val_acc: 0.7783\n",
      "Epoch 5/10\n",
      "3574/3574 [==============================] - 0s 130us/step - loss: 0.4031 - acc: 0.8671 - val_loss: 0.5736 - val_acc: 0.8096\n",
      "Epoch 6/10\n",
      "3574/3574 [==============================] - 1s 152us/step - loss: 0.3556 - acc: 0.8839 - val_loss: 0.5808 - val_acc: 0.7872\n",
      "Epoch 7/10\n",
      "3574/3574 [==============================] - 1s 142us/step - loss: 0.2932 - acc: 0.8993 - val_loss: 0.6034 - val_acc: 0.7973\n",
      "Epoch 8/10\n",
      "3574/3574 [==============================] - 1s 147us/step - loss: 0.2608 - acc: 0.9152 - val_loss: 0.5753 - val_acc: 0.8074\n",
      "Epoch 9/10\n",
      "3574/3574 [==============================] - 1s 150us/step - loss: 0.2212 - acc: 0.9284 - val_loss: 0.6710 - val_acc: 0.7917\n",
      "Epoch 10/10\n",
      "3574/3574 [==============================] - 1s 143us/step - loss: 0.1854 - acc: 0.9401 - val_loss: 0.6052 - val_acc: 0.8074\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "def reset_weights(model):\n",
    "        session = K.get_session()\n",
    "        for layer in model.layers: \n",
    "            if hasattr(layer, 'kernel_initializer'):\n",
    "                layer.kernel.initializer.run(session=session)\n",
    "                \n",
    "#for stacking\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(mlp_train, Y, stratify=Y, test_size=0.2, random_state=942)\n",
    "train_mlpfeature=np.empty([0,8])\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5,random_state=None, shuffle=False)\n",
    "for train_index, test_index in kf.split(mlp_train):\n",
    "    \n",
    "    X_train, y_train = mlp_train.iloc[train_index,:], Y[train_index]\n",
    "    X_test, y_test = mlp_train.iloc[test_index,:], Y[test_index]\n",
    "    \n",
    "    reset_weights(model)\n",
    "            \n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=32,\n",
    "              epochs=10,\n",
    "              validation_data=(X_test, y_test),\n",
    "              shuffle=True)\n",
    "    \n",
    "    train_mlpfeature=np.append(train_mlpfeature,model.predict(X_test),axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_mlpfeature=model.predict(mlp_train)\n",
    "test_mlpfeature=model.predict(mlp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2=pd.concat([train[[\"placenum\",\"open_allday\",'open_type2',\n",
    "       'open_hours_st2', 'open_days_ed2']],pd.DataFrame(train_co),pd.DataFrame(train_co2),pd.DataFrame(train_mlpfeature)],axis=1)\n",
    "test2=pd.concat([test[[\"placenum\",\"open_allday\",'open_type2',\n",
    "       'open_hours_st2', 'open_days_ed2']],pd.DataFrame(test_co),pd.DataFrame(test_co2),pd.DataFrame(test_mlpfeature)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4467, 8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mlpfeature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4467"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1/5]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's multi_error: 0.193512\n",
      "[20]\tvalid_0's multi_error: 0.189038\n",
      "[30]\tvalid_0's multi_error: 0.192394\n",
      "[40]\tvalid_0's multi_error: 0.182327\n",
      "[50]\tvalid_0's multi_error: 0.180089\n",
      "[60]\tvalid_0's multi_error: 0.177852\n",
      "[70]\tvalid_0's multi_error: 0.178971\n",
      "[80]\tvalid_0's multi_error: 0.180089\n",
      "[90]\tvalid_0's multi_error: 0.180089\n",
      "[100]\tvalid_0's multi_error: 0.182327\n",
      "Early stopping, best iteration is:\n",
      "[52]\tvalid_0's multi_error: 0.177852\n",
      "[Fold 2/5]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's multi_error: 0.177852\n",
      "[20]\tvalid_0's multi_error: 0.175615\n",
      "[30]\tvalid_0's multi_error: 0.176734\n",
      "[40]\tvalid_0's multi_error: 0.174497\n",
      "[50]\tvalid_0's multi_error: 0.170022\n",
      "[60]\tvalid_0's multi_error: 0.168904\n",
      "[70]\tvalid_0's multi_error: 0.177852\n",
      "[80]\tvalid_0's multi_error: 0.17226\n",
      "[90]\tvalid_0's multi_error: 0.176734\n",
      "Early stopping, best iteration is:\n",
      "[46]\tvalid_0's multi_error: 0.168904\n",
      "[Fold 3/5]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's multi_error: 0.183445\n",
      "[20]\tvalid_0's multi_error: 0.175615\n",
      "[30]\tvalid_0's multi_error: 0.180089\n",
      "[40]\tvalid_0's multi_error: 0.175615\n",
      "[50]\tvalid_0's multi_error: 0.175615\n",
      "[60]\tvalid_0's multi_error: 0.174497\n",
      "[70]\tvalid_0's multi_error: 0.177852\n",
      "[80]\tvalid_0's multi_error: 0.171141\n",
      "[90]\tvalid_0's multi_error: 0.171141\n",
      "[100]\tvalid_0's multi_error: 0.171141\n",
      "[110]\tvalid_0's multi_error: 0.170022\n",
      "[120]\tvalid_0's multi_error: 0.167785\n",
      "[130]\tvalid_0's multi_error: 0.168904\n",
      "[140]\tvalid_0's multi_error: 0.166667\n",
      "[150]\tvalid_0's multi_error: 0.167785\n",
      "[160]\tvalid_0's multi_error: 0.17226\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's multi_error: 0.165548\n",
      "[Fold 4/5]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's multi_error: 0.167785\n",
      "[20]\tvalid_0's multi_error: 0.167785\n",
      "[30]\tvalid_0's multi_error: 0.166667\n",
      "[40]\tvalid_0's multi_error: 0.167785\n",
      "[50]\tvalid_0's multi_error: 0.159955\n",
      "[60]\tvalid_0's multi_error: 0.161074\n",
      "[70]\tvalid_0's multi_error: 0.159955\n",
      "[80]\tvalid_0's multi_error: 0.1566\n",
      "[90]\tvalid_0's multi_error: 0.154362\n",
      "[100]\tvalid_0's multi_error: 0.1566\n",
      "[110]\tvalid_0's multi_error: 0.1566\n",
      "[120]\tvalid_0's multi_error: 0.159955\n",
      "[130]\tvalid_0's multi_error: 0.1566\n",
      "[140]\tvalid_0's multi_error: 0.157718\n",
      "[150]\tvalid_0's multi_error: 0.153244\n",
      "[160]\tvalid_0's multi_error: 0.151007\n",
      "[170]\tvalid_0's multi_error: 0.155481\n",
      "[180]\tvalid_0's multi_error: 0.154362\n",
      "[190]\tvalid_0's multi_error: 0.154362\n",
      "[200]\tvalid_0's multi_error: 0.154362\n",
      "Early stopping, best iteration is:\n",
      "[154]\tvalid_0's multi_error: 0.151007\n",
      "[Fold 5/5]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[10]\tvalid_0's multi_error: 0.182327\n",
      "[20]\tvalid_0's multi_error: 0.178971\n",
      "[30]\tvalid_0's multi_error: 0.181208\n",
      "[40]\tvalid_0's multi_error: 0.176734\n",
      "[50]\tvalid_0's multi_error: 0.174497\n",
      "Early stopping, best iteration is:\n",
      "[2]\tvalid_0's multi_error: 0.17226\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import copy\n",
    "\n",
    "params={}\n",
    "\n",
    "params={\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 8,\n",
    "    'random_state' : 1,\n",
    "    #\"objective\": \"rmse\",\n",
    "    #\"eval_metric\": \"rmse\",\n",
    "    #\"objective\": \"binary\",\n",
    "    \"metric\": \"multi_error\",\n",
    "    #'num_leaves': 50,\n",
    "    }\n",
    "\n",
    "df_train=train2\n",
    "df_label=label2\n",
    "\n",
    "p_test=0\n",
    "p_eval=0\n",
    "kfold = 5\n",
    "score=0\n",
    "sss = StratifiedShuffleSplit(n_splits=kfold, test_size=0.2, random_state=94)\n",
    "for i, (train_index, test_index) in enumerate(sss.split(df_train, df_label)):\n",
    "    print('[Fold %d/%d]' % (i + 1, kfold))\n",
    "    \n",
    "    X_train, X_valid = df_train.iloc[train_index,:], df_train.iloc[test_index,:]\n",
    "    y_train, y_valid = df_label[train_index], df_label[test_index]\n",
    "    dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "    dvalid = lgb.Dataset(X_valid, label=y_valid)\n",
    "    \n",
    "    #watchlist = [(dtrain, 'train'),(dvalid, 'eval')]\n",
    "    #watchlist = [(dvalid, 'eval')]\n",
    "    #bst = lgb.train(params, dtrain, num_boost_round=1000,valid_sets=[dtrain, dvalid],early_stopping_rounds=50,verbose_eval=10)\n",
    "    bst = lgb.train(params, dtrain, num_boost_round=1000,valid_sets=[dvalid],early_stopping_rounds=50,verbose_eval=10)\n",
    "\n",
    "    #d_test = pred_df\n",
    "    #p_test+=bst.predict(d_test)\n",
    "    p_test+=bst.predict(test2)\n",
    "    score+=bst.best_score[\"valid_0\"]['multi_error']\n",
    "    \n",
    "#p_test=p_test/5\n",
    "#p_test = np.argmax(p_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16711409395973154"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLPあり、なしより下がってる？？？\n",
    "score/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16308724832214766"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MLPなし\n",
    "score/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2073825503355705"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#charのtfidfなし\n",
    "score/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "#preds=bst.predict(pd.DataFrame(test_co))\n",
    "#preds=bst.predict(test2)\n",
    "p_test=p_test/5\n",
    "preds2=np.argmax(p_test,axis=1)\n",
    "result=le.inverse_transform(preds2)\n",
    "submit=pd.DataFrame()\n",
    "submit[\"id\"]=test[\"id\"]\n",
    "submit[\"Park\"]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv(\"park9.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
